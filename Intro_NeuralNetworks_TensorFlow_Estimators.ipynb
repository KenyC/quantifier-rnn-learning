{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks, TensorFlow, and its Estimators Interface (with an eye towards learning quantifiers)\n",
    "\n",
    "### About this notebook:\n",
    "This notebook was written by Shane Steinert-Threlkeld for the Neural Network Methods for Quantifiers coordinated project at the ILLC, Universiteit van Amsterdam in January 2018 (http://shane.st/NNQ).  \n",
    "\n",
    "It introduces the basics of working with TensorFlow to train neural networks, with an eye to applications to quantifiers.  (In particular, the code is a warm-up to understanding this repository: https://github.com/shanest/quantifier-rnn-learning.)\n",
    "\n",
    "There are three sections:\n",
    "\n",
    "1. Basic TF abstractions: sessions, the graph, Variables/Placeholders\n",
    "2. Training a feed-forward neural network to classify bit sequences\n",
    "3. Re-doing the above using TF estimators  \n",
    "\n",
    "#### Intended working environment for this notebook:\n",
    "* Python 2.7\n",
    "* Tensorflow 1.4\n",
    "\n",
    "To run: (i) install Jupyter; (ii) save this .ipynb file in a directory; (iii) from that directory, run `jupyter notebook`; (iv) open this file.\n",
    "\n",
    "### License\n",
    "Copyright 2018 Shane Steinert-Threlkeld\n",
    "\n",
    "> This program is free software: you can redistribute it and/or modify\n",
    "> it under the terms of the GNU General Public License as published by\n",
    "> the Free Software Foundation, either version 3 of the License, or\n",
    "> (at your option) any later version.\n",
    ">\n",
    "> This program is distributed in the hope that it will be useful,\n",
    "> but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "> MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "> GNU General Public License for more details.\n",
    ">\n",
    "> You should have received a copy of the GNU General Public License\n",
    "> along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TensorFlow Mechanics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and running a computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c1 = tf.constant(3.0)\n",
    "c2 = tf.constant(4.0)\n",
    "print c1\n",
    "\n",
    "add1 = tf.add(c1, c2)\n",
    "add2 = c1 + c2 #same as above, though I prefer to use the `tf.` versions of ops, to be most clear\n",
    "print add1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that what's printed is not the value 3.0, but a Tensor, a TF data-type corresponding to a node in the computational graph.\n",
    "\n",
    "To get its value, we need to _run_ the graph inside a _session_.\n",
    "\n",
    "[Note: it's always good to use a `with` block to wrap a session, so that it closes automatically.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "7.0\n",
      "[3.0, 4.0, 7.0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print sess.run(c1)\n",
    "    print sess.run(add1)\n",
    "    # you can also pass a list of ops instead of a single op to `run`\n",
    "    print sess.run([c1, c2, add1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors also have a _shape_, telling you what how many dimensions, and the size of each dimension.  I find it to be a good practice to include the shape as a comment above every operation.  Because the shape is a property of the `Tensor`, it can be accessed without running the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "[[  3.]\n",
      " [  7.]\n",
      " [ 11.]]\n"
     ]
    }
   ],
   "source": [
    "# -- mat: [3, 2]\n",
    "mat = tf.constant([[1.0, 2.0],\n",
    "                   [3.0, 4.0],\n",
    "                   [5.0, 6.0]])\n",
    "print mat.shape\n",
    "\n",
    "# -- vec: [2, 1]\n",
    "vec = tf.constant([[1.0],\n",
    "                   [1.0]])\n",
    "\n",
    "# -- mul: [3, 1]\n",
    "mul = tf.matmul(mat, vec)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and placeholders\n",
    "\n",
    "A neural network learns to approximate a given function by seeing exmples and updating its _parameters_ in order to do a better job at approximating the data it has seen.  While we fore-stall an actual discussion of training to the next section, we note two other pieces of machinery that are required for this:\n",
    "\n",
    "1. Variables: these are `Tensor`s whose values can be changed.  So parameters of a model -- and anything else you want to be updated -- will be Variables.\n",
    "2. Placeholders: these are `Tensor`s that represent input to the network/computational graph: their value must be provided externally via what TensorFlow calls a `feed_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.]\n",
      " [  8.]\n",
      " [ 12.]]\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable([[1.0, 2.0],\n",
    "                   [3.0, 4.0],\n",
    "                   [5.0, 6.0]])\n",
    "b = tf.Variable([[1.0],\n",
    "                 [1.0], \n",
    "                 [1.0]])\n",
    "\n",
    "x = tf.placeholder(shape=(2,1), dtype=tf.float32)\n",
    "\n",
    "linear = tf.matmul(W, x)\n",
    "result = tf.add(linear, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # variables must be initialized\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # result depends on a placeholder, so input must be fed in\n",
    "    print sess.run(result, feed_dict={x: [[1.0], [1.0]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the shape of the placeholder `x` was specified precisely.  While this is good practice, it's often convenient to leave one of the dimensions as `None`, so that batches of different numbers of input can be sent to the model.  (For example, mini-batches during training, one big batch during evaluation.  We'll see how this works later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training a feed-forward neural network to learn 'at least three'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating labeled data\n",
    "\n",
    "First, we will generate labeled data.  \n",
    "\n",
    "The Xs will be all sequences of 0s and 1s of a specified length.\n",
    "\n",
    "The Ys will be labels -- 0 or 1 -- provided by a user-defined function that takes a sequence as its input.  Here we provide one: `at_least_three`.\n",
    "\n",
    "The data is shuffled, so that the order is random.  Finally, it is split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools as iter\n",
    "import random\n",
    "import math\n",
    "\n",
    "def generate_all_seqs(length, shuffle=True):\n",
    "    seqs = list(iter.product([0,1], repeat=length))\n",
    "    if shuffle:\n",
    "        random.shuffle(seqs)\n",
    "    return seqs\n",
    "\n",
    "def at_least_three(seq):\n",
    "    # we return [0,1] for True and [1,0] for False\n",
    "    return [0,1] if sum(seq) >= 3 else [1,0]\n",
    "\n",
    "def get_labeled_data(seqs, func):\n",
    "    return seqs, [func(seq) for seq in seqs]\n",
    "\n",
    "# generate all labeled data\n",
    "SEQ_LEN = 16\n",
    "NUM_CLASSES = 2\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "X, Y = get_labeled_data(generate_all_seqs(SEQ_LEN), at_least_three)\n",
    "\n",
    "# split into training and test sets\n",
    "pivot_index = int(math.ceil(TRAIN_SPLIT*len(X)))\n",
    "\n",
    "trainX, trainY = X[:pivot_index], Y[:pivot_index]\n",
    "testX, testY = X[pivot_index:], Y[pivot_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network to classify sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the neural network inside a wrapper class which helps readability, separation of code components (graph building, session management/training, et cetera), and the ability to test many different models on the same data.\n",
    "\n",
    "The initializer builds a simple feed-forward neural network with one hidden layer.\n",
    "\n",
    "Instances of the class have properties for training, predicting, and evaluating, as well as for inputting sequences and labels.  These are the corresponding ops in the graph, so they can be passed directly to `Session.run()` and used in `feed_dict`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FFNN(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size=10):\n",
    "        \n",
    "        # first, basic network architecture\n",
    "        \n",
    "        # -- inputs: [batch_size, input_size]\n",
    "        inputs = tf.placeholder(shape=[None, input_size], dtype=tf.float32)\n",
    "        self._inputs = inputs\n",
    "        # -- labels: [batch_size, output_size]\n",
    "        labels = tf.placeholder(shape=[None, output_size], dtype=tf.float32)\n",
    "        self._labels = labels\n",
    "        \n",
    "        # we will have one hidden layer\n",
    "        # in general, this should be parameterized\n",
    "        \n",
    "        # -- weights1: [input_size, hidden_size]\n",
    "        weights1 = tf.Variable(tf.random_uniform(shape=[input_size, hidden_size]))\n",
    "        # -- biases1: [hidden_size]\n",
    "        biases1 = tf.Variable(tf.random_uniform(shape=[hidden_size]))\n",
    "        # -- linear: [batch_size, hidden_size]\n",
    "        linear = tf.add(tf.matmul(inputs, weights1), biases1)\n",
    "        # -- hidden: [batch_size, hidden_size]\n",
    "        hidden = tf.nn.relu(linear)\n",
    "        \n",
    "        # -- weights2: [hidden_size, output_size]\n",
    "        weights2 = tf.Variable(tf.random_uniform(shape=[hidden_size, output_size]))\n",
    "        # -- biases2: [output_size]\n",
    "        biases2 = tf.Variable(tf.random_uniform(shape=[output_size]))\n",
    "        # -- logits: [batch_size, output_size]\n",
    "        logits = tf.add(tf.matmul(hidden, weights2), biases2)\n",
    "        \n",
    "        # second, define loss and training\n",
    "        # -- cross_entropy: [batch_size]\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=labels,\n",
    "                logits=logits)\n",
    "        # -- loss: []\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self._train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        # finally, some evaluation ops\n",
    "        \n",
    "        # -- probabilities: [batch_size, output_size]\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        self._probabilities = probabilities\n",
    "        # -- predictions: [batch_size]\n",
    "        predictions = tf.argmax(probabilities, axis=1)\n",
    "        # -- targets: [batch_size]\n",
    "        targets = tf.argmax(labels, axis=1)\n",
    "        # -- correct_prediction: [batch_size]\n",
    "        correct_prediction = tf.equal(predictions, targets)\n",
    "        # -- accuracy: []\n",
    "        accuracy = tf.reduce_mean(tf.to_float(correct_prediction))\n",
    "        # more evaluation ops could be added here\n",
    "        self._eval_dict = {\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train_op\n",
    "    \n",
    "    @property\n",
    "    def predictions(self):\n",
    "        return self._probabilities\n",
    "    \n",
    "    @property\n",
    "    def evaluate(self):\n",
    "        return self._eval_dict\n",
    "    \n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self._inputs\n",
    "    \n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network\n",
    "\n",
    "Here we see the main training loop for our neural network.  There are two key parameters to training:\n",
    "* number of epochs: how many times to iterate through the whole training set\n",
    "* batch size: how large each mini-batch should be.  In other words, the network will receive this many labeled examples before computing loss and gradients and updating its parameters.\n",
    "\n",
    "In general, mini-batches of medium size strike a good balance between speed and variance.  If batch size is the size of the training set, then there's no variance in the estimate of the loss and gradients; if the batch size is 1, there's a tremendous amount of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 0, batch 0, evaluation\n",
      "{'accuracy': 0.98847944}\n",
      "\n",
      "Epoch 0, batch 50, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 100, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 150, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 200, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 250, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 300, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 350, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 400, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 450, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 500, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 550, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 600, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 650, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 700, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 750, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 800, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 850, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 900, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 950, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1000, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1050, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1100, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1150, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1200, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1250, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1300, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1350, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1400, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1450, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1500, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1550, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1600, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1650, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1700, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1750, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1800, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1850, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1900, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 1950, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2000, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2050, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2100, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2150, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2200, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2250, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2300, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2350, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2400, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2450, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2500, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2550, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2600, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2650, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2700, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2750, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2800, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2850, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2900, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 2950, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3000, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3050, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3100, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3150, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3200, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3250, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3300, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3350, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3400, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3450, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3500, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3550, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3600, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3650, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3700, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3750, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3800, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3850, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3900, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 3950, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 4000, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 4050, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 4100, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 4150, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 4200, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 4250, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 4300, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 0, batch 4350, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 0, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 50, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 100, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 150, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 200, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 250, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 300, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 350, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 400, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 450, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 500, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 550, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 600, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 650, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 700, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 750, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 800, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 850, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 900, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 950, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 1000, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 1050, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 1100, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 1150, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 1200, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 1250, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 1300, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 1350, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 1400, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 1450, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 1500, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 1550, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 1600, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 1650, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 1700, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 1750, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 1800, evaluation\n",
      "{'accuracy': 0.99771112}\n",
      "\n",
      "Epoch 1, batch 1850, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 1900, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 1950, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2000, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2050, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2100, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2150, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2200, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2250, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2300, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2350, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2400, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2450, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2500, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2550, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2600, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2650, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2700, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, batch 2750, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2800, evaluation\n",
      "{'accuracy': 0.99778742}\n",
      "\n",
      "Epoch 1, batch 2850, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 2900, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 2950, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3000, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3050, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3100, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3150, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3200, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3250, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3300, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3350, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3400, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3450, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3500, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3550, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3600, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3650, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3700, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3750, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3800, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3850, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3900, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 3950, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 4000, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 4050, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 4100, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 4150, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 4200, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 4250, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 4300, evaluation\n",
      "{'accuracy': 0.99786371}\n",
      "\n",
      "Epoch 1, batch 4350, evaluation\n",
      "{'accuracy': 0.99786371}\n"
     ]
    }
   ],
   "source": [
    "# reset the graph before building a model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # build our model\n",
    "    model = FFNN(SEQ_LEN, NUM_CLASSES)\n",
    "    # initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # MAIN TRAINING LOOP\n",
    "    NUM_EPOCHS = 2\n",
    "    BATCH_SIZE = 12\n",
    "    num_batches = len(trainX) / BATCH_SIZE\n",
    "    \n",
    "    for epoch in xrange(NUM_EPOCHS):\n",
    "        \n",
    "        # shuffle the training data at start of each epoch\n",
    "        train_data = zip(trainX, trainY)\n",
    "        random.shuffle(train_data)\n",
    "        trainX = [datum[0] for datum in train_data]\n",
    "        trainY = [datum[1] for datum in train_data]\n",
    "        \n",
    "        for batch_idx in xrange(num_batches):\n",
    "            # get batch of training data\n",
    "            batchX = trainX[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
    "            batchY = trainY[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
    "            # train on the batch\n",
    "            sess.run(model.train, \n",
    "                     {model.inputs: batchX,\n",
    "                      model.labels: batchY})\n",
    "            \n",
    "            # evaluate every N training steps (batches)\n",
    "            if batch_idx % 50 == 0:\n",
    "                print '\\nEpoch {}, batch {}, evaluation'.format(epoch, batch_idx)\n",
    "                print sess.run(model.evaluate, {model.inputs: testX, model.labels: testY})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Re-writing the above using TensorFlow Estimator\n",
    "\n",
    "The TensorFlow Estimator API -- https://www.tensorflow.org/api_docs/python/tf/estimator -- provides convenience functions that handle a lot of the nitty-gritty around running a training loop, feeding in input data, and things of that sort.\n",
    "\n",
    "Another benefit of the API: _it automatically saves and loads trained models for you_, if you use the `model_dir` argument.\n",
    "\n",
    "First, we use the library's pre-built `DNNClassifier` estimator, to show the mechanics of training and evaluating.  The basic thing to note is that we have to wrap our training and test datasets in `input_function`s, so that TensorFlow knows how to feed them to the estimator.\n",
    "\n",
    "In the next section, we will convert our `FFNN` class above into a custom-built `estimator`, to see in more detail how the API works.  This is especially important since there are not yet pre-made estimators for RNNs, so the code at https://github.com/shanest/quantifier-rnn-learning implements a custom estimator.\n",
    "\n",
    "In that next section, I will also show how to implement evaluation inside of a training loop, instead of waiting until the end of training.  This is important for our kind of experiments, which want to measure performance on the test set as training proceeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/l2/l0wvjplx49x12_mv2540hycr0000gn/T/tmpS5OaNO\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x116c8e0d0>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/l2/l0wvjplx49x12_mv2540hycr0000gn/T/tmpS5OaNO', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/l2/l0wvjplx49x12_mv2540hycr0000gn/T/tmpS5OaNO/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.67839, step = 1\n",
      "INFO:tensorflow:global_step/sec: 861.728\n",
      "INFO:tensorflow:loss = 0.781742, step = 101 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 1184.32\n",
      "INFO:tensorflow:loss = 0.172729, step = 201 (0.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 1184.73\n",
      "INFO:tensorflow:loss = 0.058866, step = 301 (0.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 1122.76\n",
      "INFO:tensorflow:loss = 0.0249032, step = 401 (0.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 1216.91\n",
      "INFO:tensorflow:loss = 0.026241, step = 501 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1223.8\n",
      "INFO:tensorflow:loss = 0.0379505, step = 601 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1221.3\n",
      "INFO:tensorflow:loss = 0.0251652, step = 701 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1209.38\n",
      "INFO:tensorflow:loss = 0.00554807, step = 801 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1212\n",
      "INFO:tensorflow:loss = 0.0313787, step = 901 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1189.64\n",
      "INFO:tensorflow:loss = 0.00471629, step = 1001 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1228.44\n",
      "INFO:tensorflow:loss = 0.0036212, step = 1101 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1206.55\n",
      "INFO:tensorflow:loss = 0.00924883, step = 1201 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1204.37\n",
      "INFO:tensorflow:loss = 0.00579495, step = 1301 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1219.25\n",
      "INFO:tensorflow:loss = 0.0157358, step = 1401 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1233.55\n",
      "INFO:tensorflow:loss = 0.00312118, step = 1501 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1212.18\n",
      "INFO:tensorflow:loss = 0.00821307, step = 1601 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1243.63\n",
      "INFO:tensorflow:loss = 0.0131504, step = 1701 (0.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 1064.54\n",
      "INFO:tensorflow:loss = 0.00499538, step = 1801 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1135.14\n",
      "INFO:tensorflow:loss = 0.0404864, step = 1901 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 1231.24\n",
      "INFO:tensorflow:loss = 0.00093313, step = 2001 (0.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 1259.99\n",
      "INFO:tensorflow:loss = 0.00250709, step = 2101 (0.080 sec)\n",
      "INFO:tensorflow:global_step/sec: 1181.33\n",
      "INFO:tensorflow:loss = 0.0108159, step = 2201 (0.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 1147.95\n",
      "INFO:tensorflow:loss = 0.0114681, step = 2301 (0.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 1173.02\n",
      "INFO:tensorflow:loss = 0.0061866, step = 2401 (0.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 1218.61\n",
      "INFO:tensorflow:loss = 0.0154907, step = 2501 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1177.88\n",
      "INFO:tensorflow:loss = 0.00244169, step = 2601 (0.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 1173.45\n",
      "INFO:tensorflow:loss = 0.00293728, step = 2701 (0.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 1178.76\n",
      "INFO:tensorflow:loss = 0.0162208, step = 2801 (0.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 1169.55\n",
      "INFO:tensorflow:loss = 0.0103443, step = 2901 (0.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 998.72\n",
      "INFO:tensorflow:loss = 0.0109137, step = 3001 (0.100 sec)\n",
      "INFO:tensorflow:global_step/sec: 1057.64\n",
      "INFO:tensorflow:loss = 0.040989, step = 3101 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 1200.97\n",
      "INFO:tensorflow:loss = 0.00178435, step = 3201 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1182.48\n",
      "INFO:tensorflow:loss = 0.00269916, step = 3301 (0.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 1071.51\n",
      "INFO:tensorflow:loss = 0.00293546, step = 3401 (0.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 1193.33\n",
      "INFO:tensorflow:loss = 0.00143122, step = 3501 (0.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 1097.48\n",
      "INFO:tensorflow:loss = 0.00166492, step = 3601 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1196.07\n",
      "INFO:tensorflow:loss = 0.0197192, step = 3701 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1209.64\n",
      "INFO:tensorflow:loss = 0.00095674, step = 3801 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1040.62\n",
      "INFO:tensorflow:loss = 0.0498387, step = 3901 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 1160.24\n",
      "INFO:tensorflow:loss = 0.0130496, step = 4001 (0.086 sec)\n",
      "INFO:tensorflow:global_step/sec: 1219.6\n",
      "INFO:tensorflow:loss = 2.35834, step = 4101 (0.082 sec)\n",
      "INFO:tensorflow:global_step/sec: 1135.51\n",
      "INFO:tensorflow:loss = 0.00189983, step = 4201 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 1196.82\n",
      "INFO:tensorflow:loss = 0.00347295, step = 4301 (0.084 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4370 into /var/folders/l2/l0wvjplx49x12_mv2540hycr0000gn/T/tmpS5OaNO/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00404066.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-19-09:33:05\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/l2/l0wvjplx49x12_mv2540hycr0000gn/T/tmpS5OaNO/model.ckpt-4370\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-19-09:33:06\n",
      "INFO:tensorflow:Saving dict for global step 4370: accuracy = 0.997711, accuracy_baseline = 0.997711, auc = 0.99993, auc_precision_recall = 1.0, average_loss = 0.00602967, global_step = 4370, label/mean = 0.997711, loss = 79.0309, prediction/mean = 0.998615\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.99771112,\n",
       " 'accuracy_baseline': 0.99771112,\n",
       " 'auc': 0.99992985,\n",
       " 'auc_precision_recall': 0.99999982,\n",
       " 'average_loss': 0.0060296678,\n",
       " 'global_step': 4370,\n",
       " 'label/mean': 0.99771112,\n",
       " 'loss': 79.030853,\n",
       " 'prediction/mean': 0.99861515}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[SEQ_LEN])]\n",
    "\n",
    "# The library has a pre-made DNNClassifier class\n",
    "classifier = tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                       hidden_units=[10],\n",
    "                                       n_classes=NUM_CLASSES,\n",
    "                                       optimizer=tf.train.AdamOptimizer())\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(trainX)},\n",
    "    # DNNClassifier wants integer labels, so take argmax of e.g. [0,1] here\n",
    "    y=np.argmax(trainY, axis=1),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=1,\n",
    "    shuffle=True)\n",
    "\n",
    "classifier.train(input_fn=train_input_fn)\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(testX)},\n",
    "    # DNNClassifier wants integer labels, so take argmax of e.g. [0,1] here\n",
    "    y=np.argmax(testY, axis=1),\n",
    "    # one big batch, instead of mini-batches\n",
    "    batch_size=len(testX),\n",
    "    shuffle=False)\n",
    "\n",
    "classifier.evaluate(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a custom estimator via a model_fn\n",
    "\n",
    "To use the `tf.estimator` library with your own models, one has to define a `model_fn`.  In this section, we convert the above `FFNN.__init__` method into such a function. I will also use `tf.layers` to simplify the code. \n",
    "\n",
    "Doing so allows one to reap the benefits of `estimator` while using novel models and/or models for which TF hasn't implemented pre-built estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/l2/l0wvjplx49x12_mv2540hycr0000gn/T/tmpS9zySs\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x181fb3b1d0>, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_master': '', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_model_dir': '/var/folders/l2/l0wvjplx49x12_mv2540hycr0000gn/T/tmpS9zySs', '_save_summary_steps': 100}\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/l2/l0wvjplx49x12_mv2540hycr0000gn/T/tmpS9zySs/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.556, step = 1\n",
      "INFO:tensorflow:global_step/sec: 797.01\n",
      "INFO:tensorflow:loss = 0.329757, step = 101 (0.125 sec)\n",
      "INFO:tensorflow:global_step/sec: 1049.38\n",
      "INFO:tensorflow:loss = 0.0865593, step = 201 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1047.96\n",
      "INFO:tensorflow:loss = 0.0356608, step = 301 (0.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 867.19\n",
      "INFO:tensorflow:loss = 0.0274032, step = 401 (0.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 1059.76\n",
      "INFO:tensorflow:loss = 0.0172729, step = 501 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 1130.47\n",
      "INFO:tensorflow:loss = 0.013911, step = 601 (0.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 896.531\n",
      "INFO:tensorflow:loss = 0.00328124, step = 701 (0.110 sec)\n",
      "INFO:tensorflow:global_step/sec: 1024.22\n",
      "INFO:tensorflow:loss = 0.0047301, step = 801 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 630.545\n",
      "INFO:tensorflow:loss = 0.00998137, step = 901 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 549.919\n",
      "INFO:tensorflow:loss = 0.0010268, step = 1001 (0.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 585.004\n",
      "INFO:tensorflow:loss = 0.00153916, step = 1101 (0.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 559.147\n",
      "INFO:tensorflow:loss = 0.00245028, step = 1201 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 557.317\n",
      "INFO:tensorflow:loss = 0.00169976, step = 1301 (0.182 sec)\n",
      "INFO:tensorflow:global_step/sec: 597.257\n",
      "INFO:tensorflow:loss = 0.000968163, step = 1401 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 481.269\n",
      "INFO:tensorflow:loss = 0.00161161, step = 1501 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 536.466\n",
      "INFO:tensorflow:loss = 0.000740919, step = 1601 (0.188 sec)\n",
      "INFO:tensorflow:global_step/sec: 469.125\n",
      "INFO:tensorflow:loss = 0.000801369, step = 1701 (0.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 614.908\n",
      "INFO:tensorflow:loss = 0.000721547, step = 1801 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 1079.06\n",
      "INFO:tensorflow:loss = 0.00148896, step = 1901 (0.093 sec)\n",
      "INFO:tensorflow:global_step/sec: 1097.44\n",
      "INFO:tensorflow:loss = 0.00190016, step = 2001 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1082.98\n",
      "INFO:tensorflow:loss = 0.000910441, step = 2101 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 1109.93\n",
      "INFO:tensorflow:loss = 0.000996522, step = 2201 (0.091 sec)\n",
      "INFO:tensorflow:global_step/sec: 1003.57\n",
      "INFO:tensorflow:loss = 0.00159625, step = 2301 (0.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 1024.27\n",
      "INFO:tensorflow:loss = 0.00297401, step = 2401 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 1031.26\n",
      "INFO:tensorflow:loss = 0.000564045, step = 2501 (0.097 sec)\n",
      "INFO:tensorflow:global_step/sec: 1091.44\n",
      "INFO:tensorflow:loss = 0.000846799, step = 2601 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 1081.06\n",
      "INFO:tensorflow:loss = 0.000438559, step = 2701 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 1115.32\n",
      "INFO:tensorflow:loss = 0.00013729, step = 2801 (0.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 1106.29\n",
      "INFO:tensorflow:loss = 0.000697601, step = 2901 (0.090 sec)\n",
      "INFO:tensorflow:global_step/sec: 1051.79\n",
      "INFO:tensorflow:loss = 0.00185794, step = 3001 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 920.997\n",
      "INFO:tensorflow:loss = 0.00158097, step = 3101 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 1084.29\n",
      "INFO:tensorflow:loss = 0.000892165, step = 3201 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 933.349\n",
      "INFO:tensorflow:loss = 0.000714025, step = 3301 (0.107 sec)\n",
      "INFO:tensorflow:global_step/sec: 1084.62\n",
      "INFO:tensorflow:loss = 0.000626512, step = 3401 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 895.968\n",
      "INFO:tensorflow:loss = 0.000245098, step = 3501 (0.112 sec)\n",
      "INFO:tensorflow:global_step/sec: 1058.02\n",
      "INFO:tensorflow:loss = 0.000303735, step = 3601 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1059.14\n",
      "INFO:tensorflow:loss = 0.000135617, step = 3701 (0.094 sec)\n",
      "INFO:tensorflow:global_step/sec: 874.28\n",
      "INFO:tensorflow:loss = 0.000276552, step = 3801 (0.114 sec)\n",
      "INFO:tensorflow:global_step/sec: 1053.5\n",
      "INFO:tensorflow:loss = 0.00118274, step = 3901 (0.095 sec)\n",
      "INFO:tensorflow:global_step/sec: 1089.27\n",
      "INFO:tensorflow:loss = 0.000390801, step = 4001 (0.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 855.93\n",
      "INFO:tensorflow:loss = 0.00055067, step = 4101 (0.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 504.49\n",
      "INFO:tensorflow:loss = 0.000423147, step = 4201 (0.205 sec)\n",
      "INFO:tensorflow:global_step/sec: 547.933\n",
      "INFO:tensorflow:loss = 0.001763, step = 4301 (0.178 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4370 into /var/folders/l2/l0wvjplx49x12_mv2540hycr0000gn/T/tmpS9zySs/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 8.98798e-05.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-19-11:14:50\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/l2/l0wvjplx49x12_mv2540hycr0000gn/T/tmpS9zySs/model.ckpt-4370\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-19-11:14:51\n",
      "INFO:tensorflow:Saving dict for global step 4370: accuracy = 0.997711, global_step = 4370, loss = 0.00696807\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.99771112, 'global_step': 4370, 'loss': 0.0069680698}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# required arguments; params will contain anything custom you want to pass to the model-building function\n",
    "def ffnn_model_fn(features, labels, mode, params):\n",
    "    \n",
    "    # basic network \n",
    "    \n",
    "    # -- inputs: [batch_size, input_size]\n",
    "    inputs = tf.to_float(features[\"x\"])\n",
    "    # -- hidden: [batch_size, hidden_size]\n",
    "    hidden = tf.layers.dense(inputs, params['hidden_size'],\n",
    "                            activation=params['hidden_activation'])\n",
    "    # -- logits: [batch_size, num_classes]\n",
    "    # note: default for tf.layers.dense is no activation, i.e. linear\n",
    "    logits = tf.layers.dense(hidden, params['num_classes'])\n",
    "    # -- cross_entropy: [batch_size]\n",
    "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=labels,\n",
    "            logits=logits)\n",
    "    # -- loss: []\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "        \n",
    "    # training\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    # it's important to pass global_step here!\n",
    "    train_op = optimizer.minimize(loss,\n",
    "                                 global_step=tf.train.get_global_step())\n",
    "    \n",
    "    # predictions\n",
    "    # -- probs: [batch_size, num_classes]\n",
    "    probs = tf.nn.softmax(logits)\n",
    "    \n",
    "    # predictions to be output; can be customized!\n",
    "    out_preds = {'probs': probs}\n",
    "    \n",
    "    # -- predictions: [batch_size]\n",
    "    predictions = tf.argmax(probs, axis=1)\n",
    "    # -- targets: [batch_size]\n",
    "    targets = tf.argmax(labels, axis=1)\n",
    "    # -- accuracy: scalar\n",
    "    accuracy = tf.metrics.accuracy(targets, predictions)\n",
    "    \n",
    "    # evaluation metrics to be output; can be customized!\n",
    "    eval_metrics = {'accuracy': accuracy}\n",
    "    \n",
    "    # return an estimator spec, specifying mode, loss, train op, predictions, and evaluation metrics\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                     loss=loss,\n",
    "                                     train_op=train_op,\n",
    "                                     predictions=out_preds,\n",
    "                                     eval_metric_ops=eval_metrics)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# hyperparameters\n",
    "hparams = {'hidden_size': 10, 'hidden_activation': tf.nn.relu, 'num_classes': 2}\n",
    "estimator = tf.estimator.Estimator(model_fn=ffnn_model_fn, params=hparams)\n",
    "\n",
    "new_train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(trainX)},\n",
    "    y=np.array(trainY),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_epochs=1,\n",
    "    shuffle=True)\n",
    "\n",
    "estimator.train(input_fn=new_train_input_fn)\n",
    "\n",
    "new_test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": np.array(testX)},\n",
    "    y=np.array(testY),\n",
    "    batch_size=len(testX),\n",
    "    shuffle=False)\n",
    "\n",
    "estimator.evaluate(input_fn=new_test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopping and continuous evaluation using SessionRunHook\n",
    "\n",
    "Using `tf.estimator.Estimator.train`, while convenient in many ways, appears to give us less control over the training loop.  When we manually managed training, it was easy to evaluate during training and to do early stopping (i.e. stop training when a certain condition is met, instead of when the entire training cycle is over).  \n",
    "\n",
    "Luckily, we can re-create these abilities using `SessionRunHook`.  While there are still disadvantages (the model has to be saved/loaded everytime you want to evaluate), the net benefits of `estimator` are positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
