{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks, TensorFlow, and its Estimators Interface (with an eye towards learning quantifiers)\n",
    "\n",
    "### About this notebook:\n",
    "This notebook was written by Shane Steinert-Threlkeld for the Neural Network Methods for Quantifiers coordinated project at the ILLC, Universiteit van Amsterdam in January 2018 (http://shane.st/NNQ).  \n",
    "\n",
    "It introduces the basics of working with TensorFlow to train neural networks, with an eye to applications to quantifiers.  (In particular, the code is a warm-up to understanding this repository: https://github.com/shanest/quantifier-rnn-learning.)\n",
    "\n",
    "There are three sections:\n",
    "\n",
    "1. Basic TF abstractions: sessions, the graph, Variables/Placeholders\n",
    "2. Training a feed-forward neural network to classify bit sequences\n",
    "3. Re-doing the above using TF estimators  \n",
    "\n",
    "#### Intended working environment for this notebook:\n",
    "* Python 2.7\n",
    "* Tensorflow 1.4\n",
    "\n",
    "To run: (i) install Jupyter; (ii) save this .ipynb file in a directory; (iii) from that directory, run `jupyter notebook`; (iv) open this file.\n",
    "\n",
    "### License\n",
    "Copyright 2018 Shane Steinert-Threlkeld\n",
    "\n",
    "> This program is free software: you can redistribute it and/or modify\n",
    "> it under the terms of the GNU General Public License as published by\n",
    "> the Free Software Foundation, either version 3 of the License, or\n",
    "> (at your option) any later version.\n",
    ">\n",
    "> This program is distributed in the hope that it will be useful,\n",
    "> but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "> MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "> GNU General Public License for more details.\n",
    ">\n",
    "> You should have received a copy of the GNU General Public License\n",
    "> along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. TensorFlow Mechanics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining and running a computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "c1 = tf.constant(3.0)\n",
    "c2 = tf.constant(4.0)\n",
    "print c1\n",
    "\n",
    "add1 = tf.add(c1, c2)\n",
    "add2 = c1 + c2 #same as above, though I prefer to use the `tf.` versions of ops, to be most clear\n",
    "print add1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that what's printed is not the value 3.0, but a Tensor, a TF data-type corresponding to a node in the computational graph.\n",
    "\n",
    "To get its value, we need to _run_ the graph inside a _session_.\n",
    "\n",
    "[Note: it's always good to use a `with` block to wrap a session, so that it closes automatically.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "7.0\n",
      "[3.0, 4.0, 7.0]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print sess.run(c1)\n",
    "    print sess.run(add1)\n",
    "    # you can also pass a list of ops instead of a single op to `run`\n",
    "    print sess.run([c1, c2, add1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors also have a _shape_, telling you what how many dimensions, and the size of each dimension.  I find it to be a good practice to include the shape as a comment above every operation.  Because the shape is a property of the `Tensor`, it can be accessed without running the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "[[  3.]\n",
      " [  7.]\n",
      " [ 11.]]\n"
     ]
    }
   ],
   "source": [
    "# -- mat: [3, 2]\n",
    "mat = tf.constant([[1.0, 2.0],\n",
    "                   [3.0, 4.0],\n",
    "                   [5.0, 6.0]])\n",
    "print mat.shape\n",
    "\n",
    "# -- vec: [2, 1]\n",
    "vec = tf.constant([[1.0],\n",
    "                   [1.0]])\n",
    "\n",
    "# -- mul: [3, 1]\n",
    "mul = tf.matmul(mat, vec)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(mul)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables and placeholders\n",
    "\n",
    "A neural network learns to approximate a given function by seeing exmples and updating its _parameters_ in order to do a better job at approximating the data it has seen.  While we fore-stall an actual discussion of training to the next section, we note two other pieces of machinery that are required for this:\n",
    "\n",
    "1. Variables: these are `Tensor`s whose values can be changed.  So parameters of a model -- and anything else you want to be updated -- will be Variables.\n",
    "2. Placeholders: these are `Tensor`s that represent input to the network/computational graph: their value must be provided externally via what TensorFlow calls a `feed_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.]\n",
      " [  8.]\n",
      " [ 12.]]\n"
     ]
    }
   ],
   "source": [
    "W = tf.Variable([[1.0, 2.0],\n",
    "                   [3.0, 4.0],\n",
    "                   [5.0, 6.0]])\n",
    "b = tf.Variable([[1.0],\n",
    "                 [1.0], \n",
    "                 [1.0]])\n",
    "\n",
    "x = tf.placeholder(shape=(2,1), dtype=tf.float32)\n",
    "\n",
    "linear = tf.matmul(W, x)\n",
    "result = tf.add(linear, b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # variables must be initialized\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # result depends on a placeholder, so input must be fed in\n",
    "    print sess.run(result, feed_dict={x: [[1.0], [1.0]]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the shape of the placeholder `x` was specified precisely.  While this is good practice, it's often convenient to leave one of the dimensions as `None`, so that batches of different numbers of input can be sent to the model.  (For example, mini-batches during training, one big batch during evaluation.  We'll see how this works later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training a feed-forward neural network to learn 'at least three'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating labeled data\n",
    "\n",
    "First, we will generate labeled data.  \n",
    "\n",
    "The Xs will be all sequences of 0s and 1s of a specified length.\n",
    "\n",
    "The Ys will be labels -- 0 or 1 -- provided by a user-defined function that takes a sequence as its input.  Here we provide one: `at_least_three`.\n",
    "\n",
    "The data is shuffled, so that the order is random.  Finally, it is split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as iter\n",
    "import random\n",
    "import math\n",
    "\n",
    "def generate_all_seqs(length, shuffle=True):\n",
    "    seqs = list(iter.product([0,1], repeat=length))\n",
    "    if shuffle:\n",
    "        random.shuffle(seqs)\n",
    "    return seqs\n",
    "\n",
    "def at_least_three(seq):\n",
    "    # we return [0,1] for True and [1,0] for False\n",
    "    return [0,1] if sum(seq) >= 3 else [1,0]\n",
    "\n",
    "def get_labeled_data(seqs, func):\n",
    "    return seqs, [func(seq) for seq in seqs]\n",
    "\n",
    "# generate all labeled data\n",
    "SEQ_LEN = 16\n",
    "NUM_CLASSES = 2\n",
    "TRAIN_SPLIT = 0.8\n",
    "\n",
    "X, Y = get_labeled_data(generate_all_seqs(SEQ_LEN), at_least_three)\n",
    "\n",
    "# split into training and test sets\n",
    "pivot_index = int(math.ceil(TRAIN_SPLIT*len(X)))\n",
    "\n",
    "trainX, trainY = X[:pivot_index], Y[:pivot_index]\n",
    "testX, testY = X[pivot_index:], Y[pivot_index:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network to classify sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the neural network inside a wrapper class which helps readability, separation of code components (graph building, session management/training, et cetera), and the ability to test many different models on the same data.\n",
    "\n",
    "The initializer builds a simple feed-forward neural network with one hidden layer.\n",
    "\n",
    "Instances of the class have properties for training, predicting, and evaluating, as well as for inputting sequences and labels.  These are the corresponding ops in the graph, so they can be passed directly to `Session.run()` and used in `feed_dict`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FFNN(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size=10):\n",
    "        \n",
    "        # first, basic network architecture\n",
    "        \n",
    "        # -- inputs: [batch_size, input_size]\n",
    "        inputs = tf.placeholder(shape=[None, input_size], dtype=tf.float32)\n",
    "        self._inputs = inputs\n",
    "        # -- labels: [batch_size, output_size]\n",
    "        labels = tf.placeholder(shape=[None, output_size], dtype=tf.float32)\n",
    "        self._labels = labels\n",
    "        \n",
    "        # we will have one hidden layer\n",
    "        # in general, this should be parameterized\n",
    "        \n",
    "        # -- weights1: [input_size, hidden_size]\n",
    "        weights1 = tf.Variable(tf.random_uniform(shape=[input_size, hidden_size]))\n",
    "        # -- biases1: [hidden_size]\n",
    "        biases1 = tf.Variable(tf.random_uniform(shape=[hidden_size]))\n",
    "        # -- linear: [batch_size, hidden_size]\n",
    "        linear = tf.add(tf.matmul(inputs, weights1), biases1)\n",
    "        # -- hidden: [batch_size, hidden_size]\n",
    "        hidden = tf.nn.relu(linear)\n",
    "        \n",
    "        # -- weights2: [hidden_size, output_size]\n",
    "        weights2 = tf.Variable(tf.random_uniform(shape=[hidden_size, output_size]))\n",
    "        # -- biases2: [output_size]\n",
    "        biases2 = tf.Variable(tf.random_uniform(shape=[output_size]))\n",
    "        # -- logits: [batch_size, output_size]\n",
    "        logits = tf.add(tf.matmul(hidden, weights2), biases2)\n",
    "        \n",
    "        # second, define loss and training\n",
    "        # -- cross_entropy: [batch_size]\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=labels,\n",
    "                logits=logits)\n",
    "        # -- loss: []\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        self._train_op = optimizer.minimize(loss)\n",
    "        \n",
    "        # finally, some evaluation ops\n",
    "        \n",
    "        # -- probabilities: [batch_size, output_size]\n",
    "        probabilities = tf.nn.softmax(logits)\n",
    "        self._probabilities = probabilities\n",
    "        # -- predictions: [batch_size]\n",
    "        predictions = tf.argmax(probabilities, axis=1)\n",
    "        # -- targets: [batch_size]\n",
    "        targets = tf.argmax(labels, axis=1)\n",
    "        # -- correct_prediction: [batch_size]\n",
    "        correct_prediction = tf.equal(predictions, targets)\n",
    "        # -- accuracy: []\n",
    "        accuracy = tf.reduce_mean(tf.to_float(correct_prediction))\n",
    "        # more evaluation ops could be added here\n",
    "        self._eval_dict = {\n",
    "            'accuracy': accuracy\n",
    "        }\n",
    "        \n",
    "    @property\n",
    "    def train(self):\n",
    "        return self._train_op\n",
    "    \n",
    "    @property\n",
    "    def predictions(self):\n",
    "        return self._probabilities\n",
    "    \n",
    "    @property\n",
    "    def evaluate(self):\n",
    "        return self._eval_dict\n",
    "    \n",
    "    @property\n",
    "    def inputs(self):\n",
    "        return self._inputs\n",
    "    \n",
    "    @property\n",
    "    def labels(self):\n",
    "        return self._labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "At end of epoch 0\n",
      "{'accuracy': 0.99816895}\n",
      "\n",
      "At end of epoch 1\n",
      "{'accuracy': 0.99946594}\n",
      "\n",
      "At end of epoch 2\n",
      "{'accuracy': 1.0}\n",
      "\n",
      "At end of epoch 3\n",
      "{'accuracy': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# reset the graph before building a model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # build our model\n",
    "    model = FFNN(SEQ_LEN, NUM_CLASSES)\n",
    "    # initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # MAIN TRAINING LOOP\n",
    "    NUM_EPOCHS = 4\n",
    "    BATCH_SIZE = 8\n",
    "    num_batches = len(trainX) / BATCH_SIZE\n",
    "    \n",
    "    for epoch in xrange(NUM_EPOCHS):\n",
    "        \n",
    "        # shuffle the training data at start of each epoch\n",
    "        train_data = zip(trainX, trainY)\n",
    "        random.shuffle(train_data)\n",
    "        trainX = [datum[0] for datum in train_data]\n",
    "        trainY = [datum[1] for datum in train_data]\n",
    "        \n",
    "        for batch_idx in xrange(num_batches):\n",
    "            # get batch of training data\n",
    "            batchX = trainX[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
    "            batchY = trainY[batch_idx*BATCH_SIZE:(batch_idx+1)*BATCH_SIZE]\n",
    "            # train on the batch\n",
    "            sess.run(model.train, \n",
    "                     {model.inputs: batchX,\n",
    "                      model.labels: batchY})\n",
    "            \n",
    "        # evaluate at end of each epoch; this can also be done more often\n",
    "        print '\\nAt end of epoch {}'.format(epoch)\n",
    "        print sess.run(model.evaluate, {model.inputs: testX, model.labels: testY})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Re-writing the above using TensorFlow Estimator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
